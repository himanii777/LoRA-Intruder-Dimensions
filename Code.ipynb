{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======GPU assign=====#\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('Current cuda device: ', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====Packages====#\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM, #(Automatically loads a model for causal language modeling)\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TrainerCallback\n",
    "    \n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, PeftConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Check compatibility for precision training=======#\n",
    "\n",
    "# gpu_name = torch.cuda.get_device_name(0)\n",
    "# compute_capability = torch.cuda.get_device_capability(0)\n",
    "# print(f\"GPU Name: {gpu_name}\")\n",
    "# print(f\"Compute Capability: {compute_capability}\")\n",
    "\n",
    "\"\"\"\n",
    "If greater than 7 then we can set fp16 to True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Model info======#\n",
    "\n",
    "model_name = \"roberta-base\"  # RoBERTa for MNLI task\n",
    "task_name = \"mnli\"\n",
    "num_labels=3  # MNLI has three labels: entailment, contradiction, neutral\n",
    "output_dir = \"/home/himani/Roberta_LoRA_mnli\"\n",
    "fine_tuned_model_path = \"/home/himani/Roberta_LoRA_mnli/Experiment_3_epoch_3\" #later for evaluation\n",
    "plot_file_name=\"Experiment_4_similarity_matrices\"\n",
    "csv_name=\"Experiment_4_intruders.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======Load dataset=======#\n",
    "\n",
    "# Load MNLI dataset\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\") \n",
    "\n",
    "# Limit training dataset \n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(50000))\n",
    "validation_dataset = dataset[\"validation_matched\"]\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"],  # MNLI premise\n",
    "        examples[\"hypothesis\"],  # MNLI hypothesis\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,  \n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Prepare for Trainer\n",
    "train_dataset = train_dataset.remove_columns([\"idx\", \"premise\", \"hypothesis\"])  # Remove unused columns\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")  # Rename\n",
    "train_dataset.set_format(\"torch\")  \n",
    "\n",
    "validation_dataset = validation_dataset.remove_columns([\"idx\", \"premise\", \"hypothesis\"])\n",
    "validation_dataset = validation_dataset.rename_column(\"label\", \"labels\")\n",
    "validation_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataset.set_format(\"torch\", device=\"cuda:0\")\n",
    "validation_dataset.set_format(\"torch\", device=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Load LLM======#\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", \n",
    "    num_labels=3,  # MNLI has three labels: entailment, contradiction, neutral\n",
    ")\n",
    "\n",
    "# tokenizer for RoBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility with padding\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "# print(f\"Padding token: {tokenizer.pad_token}, EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "# configure LoRA for sequence classification\n",
    "peft_config = LoraConfig(\n",
    "    r=1, \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",  \n",
    "    task_type=\"SEQ_CLS\",  # sequence classification\n",
    ")\n",
    "\n",
    "scaling_factor = 1/ 256\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(\"cuda:0\")\n",
    "\n",
    "\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "\n",
    "# Check if we imported trainable LoRA\n",
    "# print(\"LoRA layers in the model:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"lora\" in name.lower():\n",
    "#         print(f\"{name}: Trainable = {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======Set training parameters=======#\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_strategy=\"epoch\",  \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    max_grad_norm=1.0,  # 0.3 if needed\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=None\n",
    ")\n",
    "\n",
    "\n",
    "# Customize Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        # Move all inputs to cuda:0\n",
    "        inputs = {k: v.to(\"cuda:0\")  for k, v in inputs.items()}\n",
    "        return super().training_step(model, inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======Start Training======#\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # Fine-tuned model\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Extract lora_A and lora_B for delta weight======#\n",
    "\n",
    "def extract_delta_weights(base_model_name, model_path, scaling_factor):\n",
    "    \"\"\"\n",
    "   (W_delta = scaling_factor * lora_B @ lora_A) from a fine-tuned model\n",
    "   model path= finetuned model path\n",
    "   to load the finetuned model, you also need to pass the base model in the arg\n",
    "    \"\"\"\n",
    "\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=3)   \n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    delta_weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_A\" in name:\n",
    "            layer_base_name = name.replace(\".lora_A.default.weight\", \"\")\n",
    "            lora_a = param.clone().detach().to(\"cuda\")\n",
    "            \n",
    "            lora_b_name = layer_base_name + \".lora_B.default.weight\"\n",
    "            if lora_b_name in model.state_dict():\n",
    "                lora_b = model.state_dict()[lora_b_name].clone().detach().to(\"cuda\")\n",
    "                delta_w = scaling_factor * torch.matmul(lora_b, lora_a)\n",
    "                delta_weights.append(delta_w)\n",
    "            else:\n",
    "                print(f\"Missing corresponding LoRA B weight for {name}\")\n",
    "    \n",
    "    print(f\"Extracted delta weights for {len(delta_weights)} layers.\")\n",
    "    return delta_weights\n",
    "\n",
    "\n",
    "delta_weights = extract_delta_weights(model_name, fine_tuned_model_path, scaling_factor)\n",
    "# for idx, weight in enumerate(delta_weights):\n",
    "#     print(f\"Layer {idx}: Shape = {weight.shape}, Device = {weight.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========Extract weight from the basemodel i.e. W0========#\n",
    "\n",
    "def extract_weights_from_base_model(model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    We are only interested in the query, value weights of each layer since LoRA only changes q,v \n",
    "    \"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # extract query and value weights with names\n",
    "    extracted_weights = []\n",
    "    extracted_layer_names = [] # we will use the names for plots\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"attention.self.query.weight\" in name or \"attention.self.value.weight\" in name:\n",
    "            extracted_weights.append(param.clone().detach())\n",
    "            extracted_layer_names.append(name)  # Save the layer name\n",
    "    \n",
    "    # print(f\"Extracted {len(extracted_weights)} layers from base model ({base_model_name}):\")\n",
    "    # for layer_name in extracted_layer_names:\n",
    "    #     print(layer_name)\n",
    "\n",
    "    return extracted_weights, extracted_layer_names\n",
    "\n",
    "base_weights, layer_names = extract_weights_from_base_model(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Check if we have extracted tensor weights======#\n",
    "\n",
    "# print(f\"Base weights type: {[type(weight) for weight in base_weights]}\")\n",
    "# print(f\"Type of each extracted weight: {[type(w) for w in delta_weights]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======Metric Functions=======#\n",
    "\n",
    "def cosine_similarity_matrix(W1, W2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    U1, _, _ = torch.svd(W1)  \n",
    "    U2, _, _ = torch.svd(W2)  \n",
    "    cos_sim_matrix = torch.mm(U1.T, U2)\n",
    "    return cos_sim_matrix \n",
    "\n",
    "\n",
    "def count_intruder_dimensions(similarity_matrix, epsilon=0.6):\n",
    "    \"\"\"\n",
    "    Count intruder dimensions from the similarity matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(similarity_matrix, torch.Tensor):\n",
    "        similarity_matrix = similarity_matrix.detach().cpu().numpy()\n",
    "    elif not isinstance(similarity_matrix, np.ndarray):\n",
    "        raise ValueError(\"similarity_matrix must be a torch.Tensor or a NumPy array.\")\n",
    "    \n",
    "    max_similarities = np.max(similarity_matrix, axis=1)\n",
    "    intruder_count = np.sum(max_similarities < epsilon)\n",
    "    return intruder_count\n",
    "\n",
    "\n",
    "def plot_max_similarity_checkerboard(W1, W2, save_path=None, epoch=None, layer=None, top_k=50):\n",
    "    \"\"\"\n",
    "    Compute and plot the similarity matrix with masked max similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = W1.to(\"cuda\")\n",
    "    W2 = W2.to(\"cuda\")\n",
    "\n",
    "    U1, S1, _ = torch.svd(W1)\n",
    "    U2, S2, _ = torch.svd(W2)\n",
    "\n",
    "    # sort singular vectors\n",
    "    U1_sorted = U1[:, torch.argsort(S1, descending=True)]\n",
    "    U2_sorted = U2[:, torch.argsort(S2, descending=True)]\n",
    "\n",
    "    similarity_matrix = torch.mm(U1_sorted.T, U2_sorted)\n",
    "\n",
    "    # select top_k singular vectors\n",
    "    similarity_matrix = similarity_matrix[:top_k, :top_k]\n",
    "\n",
    "    # mask max similarities\n",
    "    max_similarity_matrix = torch.zeros_like(similarity_matrix, device=W1.device)\n",
    "    max_indices = torch.argmax(similarity_matrix, dim=1)\n",
    "    max_similarity_matrix[torch.arange(similarity_matrix.size(0), device=W1.device), max_indices] = \\\n",
    "        similarity_matrix[torch.arange(similarity_matrix.size(0), device=W1.device), max_indices]\n",
    "    max_similarity_matrix_np = max_similarity_matrix.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(max_similarity_matrix_np, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "    plt.colorbar(label=\"Cosine Similarity\")\n",
    "    plt.xlabel(f\"Singular Vectors in $W_{{tuned}}$\")\n",
    "    plt.ylabel(f\"Singular Vectors in $W_{{0}}$\")\n",
    "    title = f\"Similarity Matrix: Epoch{epoch}_{layer}\"\n",
    "    plt.title(title)\n",
    "\n",
    "    # Save or show the plot\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved similarity plot to: {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Metrics Class=======#\n",
    "\n",
    "class MetricsEvaluator:\n",
    "    def __init__(self, base_weights, delta_weights, layer_names,plot_file_name,csv_name, log_dir=\"./logs\"):\n",
    "\n",
    "        self.base_weights = [weight.to(\"cuda\") for weight in base_weights]\n",
    "        self.delta_weights = [weight.to(\"cuda\") for weight in delta_weights]\n",
    "        self.layer_names = [\n",
    "            name.replace(\"encoder.\", \"\") for name in layer_names\n",
    "        ]  # remove \"encoder.\" prefix for cleaner names, these layer names are used for plotting title\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        self.similarity_matrices_dir = os.path.join(log_dir,plot_file_name )\n",
    "        self.log_file = os.path.join(log_dir, csv_name)\n",
    "\n",
    "        os.makedirs(self.similarity_matrices_dir, exist_ok=True)\n",
    "        if not os.path.exists(self.log_file):\n",
    "            with open(self.log_file, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"epoch\", \"layer\", \"intruder_count\"])\n",
    "\n",
    "    def calculate_similarity_matrix(self, base_weight, delta_weight):\n",
    "        fine_tuned_weight = base_weight + delta_weight\n",
    "        return cosine_similarity_matrix(base_weight, fine_tuned_weight)\n",
    "\n",
    "    def count_intruders(self, similarity_matrix, epsilon=0.6):\n",
    "        return count_intruder_dimensions(similarity_matrix, epsilon)\n",
    "\n",
    "    def plot_similarity(self, base_weight, delta_weight, epoch, layer_name):\n",
    "        \"\"\"\n",
    "        plot the similarity matrix for a specific layer.\n",
    "        \"\"\"\n",
    "        plot_file = os.path.join(self.similarity_matrices_dir, f\"epoch_{epoch}_{layer_name}.png\")\n",
    "        fine_tuned_weight = base_weight + delta_weight\n",
    "\n",
    "        plot_max_similarity_checkerboard(\n",
    "            base_weight,\n",
    "            fine_tuned_weight,\n",
    "            save_path=plot_file,\n",
    "            epoch=epoch,\n",
    "            layer=layer_name,\n",
    "        )\n",
    "        print(f\"Saved similarity plot for {layer_name} at epoch {epoch} to {plot_file}\")\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        \"\"\"\n",
    "        we need the epoch to write in csv, I am saving models at each epochs so..\n",
    "        \"\"\"\n",
    "        assert len(self.base_weights) == len(self.delta_weights) == len(self.layer_names), (\n",
    "            \"Base weights, delta weights, and layer names must all have the same length!\"\n",
    "        )\n",
    "\n",
    "        for i, (base_weight, delta_weight) in enumerate(zip(self.base_weights, self.delta_weights)):\n",
    "            layer_name = self.layer_names[i]  #for plotting\n",
    "            try:\n",
    "                similarity_matrix = self.calculate_similarity_matrix(base_weight, delta_weight) #finetuned weight is retrieved in the fn later\n",
    "                intruder_count = self.count_intruders(similarity_matrix)\n",
    "\n",
    "                # save intruder count to CSV\n",
    "                with open(self.log_file, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([epoch, layer_name, intruder_count])\n",
    "\n",
    "                self.plot_similarity(base_weight, delta_weight, epoch, layer_name)\n",
    "                print(f\"Processed {layer_name}: Intruder Count = {intruder_count}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {layer_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======Get the metrics======#\n",
    "\n",
    "metrics_evaluator = MetricsEvaluator(\n",
    "    base_weights=base_weights,\n",
    "    delta_weights=delta_weights,\n",
    "    layer_names=layer_names,  # pass the layer names\n",
    "    plot_file_name=plot_file_name,\n",
    "    csv_name=csv_name,\n",
    "    log_dir=\"./logs\"\n",
    ")\n",
    "metrics_evaluator.evaluate(epoch=3) #write the epoch of the saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========Load benchmarks=======#\n",
    "\n",
    "benchmarks = {\n",
    "    \"mnli_matched\": load_dataset(\"glue\", \"mnli\", split=\"validation_matched\"),\n",
    "    \"mnli_mismatched\": load_dataset(\"glue\", \"mnli\", split=\"validation_mismatched\"),\n",
    "    \"snli\": load_dataset(\"snli\", split=\"test\"),\n",
    "    \"hans\": load_dataset(\"hans\", split=\"validation\")\n",
    "}\n",
    "\n",
    "# ======== Preprocessing Function ======== #\n",
    "def preprocess_eval_function(examples, dataset_name, tokenizer):\n",
    "    if dataset_name in [\"mnli_matched\", \"mnli_mismatched\", \"snli\", \"hans\"]:\n",
    "        return tokenizer(\n",
    "            examples[\"premise\"], examples[\"hypothesis\"],\n",
    "            truncation=True, padding=\"max_length\", max_length=128\n",
    "        )\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "\n",
    "# ======== Evaluation Function ======== #\n",
    "\n",
    "def evaluate_model_from_path(model_path, benchmarks, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluates a fine-tuned LoRA model on given benchmarks, computing separate average accuracy scores\n",
    "    for in-distribution and out-of-distribution tasks.\n",
    "    \"\"\"\n",
    "    # Load LoRA adapter configuration\n",
    "    adapter_config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "    # Load the base model and tokenizer\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        adapter_config.base_model_name_or_path,\n",
    "        num_labels=3  # Assuming 3 labels for MNLI-like tasks\n",
    "    ).to(\"cuda:0\")\n",
    "    model = PeftModel.from_pretrained(base_model, model_path).eval()  # Set to eval mode\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_config.base_model_name_or_path)\n",
    "\n",
    "    # Separate benchmarks into in-distribution and out-of-distribution\n",
    "    in_distribution = [\"mnli_matched\"]\n",
    "    out_of_distribution = [\"mnli_mismatched\", \"snli\", \"hans\"]\n",
    "\n",
    "    in_acc_total, out_acc_total = 0.0, 0.0\n",
    "    in_count, out_count = 0, 0\n",
    "\n",
    "    for dataset_name, dataset in benchmarks.items():\n",
    "        print(f\"Evaluating on {dataset_name}...\")\n",
    "\n",
    "        tokenized_dataset = dataset.map(\n",
    "            lambda x: preprocess_eval_function(x, dataset_name, tokenizer),\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset.column_names if col not in [\"label\"]]\n",
    "        )\n",
    "        if \"label\" in tokenized_dataset.column_names:\n",
    "            tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "        tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            tokenized_dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=lambda x: {\n",
    "                key: torch.stack([example[key] for example in x]).to(\"cuda:0\")\n",
    "                for key in [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "            },\n",
    "        )\n",
    "\n",
    "        all_predictions, all_labels = [], []\n",
    "\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: batch[k] for k in [\"input_ids\", \"attention_mask\"]}\n",
    "            labels = batch[\"labels\"]\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "        # Accumulate accuracy scores based on distribution type\n",
    "        if dataset_name in in_distribution:\n",
    "            in_acc_total += acc\n",
    "            in_count += 1\n",
    "        elif dataset_name in out_of_distribution:\n",
    "            out_acc_total += acc\n",
    "            out_count += 1\n",
    "\n",
    "    # Calculate average accuracy for in-distribution and out-of-distribution\n",
    "    avg_in_acc = in_acc_total / in_count if in_count > 0 else 0.0\n",
    "    avg_out_acc = out_acc_total / out_count if out_count > 0 else 0.0\n",
    "\n",
    "    print(f\"Average In-Distribution Accuracy: {avg_in_acc:.4f}\")\n",
    "    print(f\"Average Out-of-Distribution Accuracy: {avg_out_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"avg_in_distribution_accuracy\": avg_in_acc,\n",
    "        \"avg_out_of_distribution_accuracy\": avg_out_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_from_path(fine_tuned_model_path, benchmarks, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
